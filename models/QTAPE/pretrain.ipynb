{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908e0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/python/DeepModelFusion/ml4quantum/dataset_generation\n",
      "/home/ubuntu/code/python/DeepModelFusion/ml4quantum/dataset_generation\n"
     ]
    }
   ],
   "source": [
    "# 基于当前工作目录\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "parent_parent_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "target_folder_path = os.path.join(parent_parent_dir, \"dataset_generation\")\n",
    "print(target_folder_path)\n",
    "# 基于脚本文件位置\n",
    "# import os\n",
    "# script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# project_root = os.path.dirname(os.path.dirname(script_dir))\n",
    "# target_folder_path = os.path.join(project_root, \"dataset_generation\")\n",
    "import utils\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from decoder import Decoder\n",
    "import embedding \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "samples_num = 100 \n",
    "shots_num = 1024\n",
    "qubits_num = 8 # L\n",
    "batch_size = 64 #100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_seed(2024)\n",
    "try:\n",
    "    dataset_path = \"/heisenberg_1d/n{samples_num}|X(coupling, meas{shots})_y(energy,entropy,corrs)_q{q}.csv\".format(samples_num=samples_num, shots=shots_num, q=qubits_num)\n",
    "    df = pd.read_csv(target_folder_path + dataset_path)\n",
    "except:\n",
    "    raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "meas_records = np.array([utils.read_matrix_v2(x) for x in df['measurement_samples'].values]) # shape (samples_num, shots_num * qubits_num)\n",
    "conditions = np.array([utils.read_matrix_v2(x) for x in df['coupling_matrix'].values]) # shape (samples_num, qubits_num * qubits_num)\n",
    "\n",
    "meas_records = meas_records.reshape(-1, shots_num, qubits_num) # shape (samples_num, shots_num, qubits_num)\n",
    "meas_records = meas_records.reshape(-1, qubits_num) # shape (samples_num * shots_num, qubits_num)\n",
    "\n",
    "new_conditions = []\n",
    "for i in range(samples_num):\n",
    "    for _ in range(shots_num):\n",
    "        new_conditions.append(conditions[i])\n",
    "new_conditions = np.array(new_conditions) # shape (samples_num * shots_num, qubits_num * qubits_num)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_dim = 512\n",
    "seq_len = qubits_num + 1 # +1 for the CLS token, and the CLS token is at the first position of the sequence\n",
    "\n",
    "batch_conditions, batch_measures = [], []\n",
    "sample_idx = np.random.choice(range(samples_num*shots_num), batch_size, replace=False)\n",
    "batch_measures = meas_records[sample_idx]   # shape (batch_size, qubits_num)\n",
    "batch_conditions = new_conditions[sample_idx]   # shape (batch_size, qubits_num * qubits_num)\n",
    "all_embeddings, token_embedding = embedding.get_embedding(batch_size, seq_len, embedding_dim, batch_measures, batch_conditions)\n",
    "# shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "embeddings = all_embeddings\n",
    "labels = F.softmax(token_embedding, dim=-1)\n",
    "dataset = TensorDataset(embeddings, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "#？？？？？？？？？？？为什么数据集这样安排，特征是位置 实验条件 和测量结果的嵌入，标签是测量结果的概率分布？？？？？？？？？？\n",
    "\n",
    "decoder = Decoder(embedding_dim, seq_len, embedding_dim, ffn_hidden=128, n_head=8, n_layers=qubits_num, drop_prob=0.1, device='cuda')\n",
    "# output: (batch_size, seq_len, embedding_dim) 最后一维是词表大小，概率分布形式\n",
    "criterion = nn.KLDivLoss(reduction='batchmean') # 对批量和序列长度维度求平均，保留嵌入维度的差异。\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89521c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "trg_mask = torch.tril(torch.ones(3, 3))\n",
    "print(trg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a5b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "trg_mask = torch.ones(3,3)\n",
    "print(trg_mask)\n",
    "trg_mask = torch.triu(trg_mask, diagonal=1)\n",
    "print(trg_mask)\n",
    "trg_mask = trg_mask.masked_fill(trg_mask == 1, float(0))\n",
    "print(trg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    decoder.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        trg_mask = torch.ones(8, all_embeddings.shape[1], all_embeddings.shape[1])\n",
    "        trg_mask = torch.triu(trg_mask, diagonal=1)\n",
    "        trg_mask = 1-trg_mask\n",
    "        #trg_mask = trg_mask.masked_fill(trg_mask == 1, float(0))\n",
    "        #?????????????????????????变为全0矩阵了，实际上应该是上三角部分为-inf，下三角部分为0的矩阵??????????????????????\n",
    "        #trg_mask = trg_mask.to(device)\n",
    "        outputs = decoder(inputs, trg_mask) # shape: (batch_size, seq_len, embedding_dim)概率分布\n",
    "\n",
    "        loss = criterion(outputs.contiguous().view(-1, all_embeddings.size(-1)), targets.contiguous().view(-1, all_embeddings.size(-1)))\n",
    "        # 展开成二维张量（batch_size * seq_len, embedding_dim），逐个位置计算KL散度\n",
    "        #？？？？？？？？？？？？？？这里为什么要手动处理，而不是loss.backward()？？？？？？？？？？？？？？？\n",
    "        gradients = torch.autograd.grad(loss, decoder.parameters(), retain_graph=True)\n",
    "        for param, grad in zip(decoder.parameters(), gradients):\n",
    "            param.grad = grad\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.8f}')\n",
    "\n",
    "# save the decoder model\n",
    "pretrain_path = \"save/pretrain_q{}_s{}_bs{}_ep{}.pt\".format(qubits_num, shots_num, batch_size, epochs)\n",
    "torch.save(decoder.state_dict(), pretrain_path)\n",
    "print(\"Pretrained model saved at\", pretrain_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afdc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
