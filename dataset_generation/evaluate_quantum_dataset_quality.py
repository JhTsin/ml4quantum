#!/usr/bin/env python3
"""
é€šç”¨é‡å­å¤šä½“æ•°æ®é›†è´¨é‡è¯„ä¼°è„šæœ¬
============================

æ”¯æŒè¯„ä¼°æ‰€æœ‰é‡å­å¤šä½“æ¨¡å‹çš„æ•°æ®é›†è´¨é‡ï¼š
- 2Dæ¨ªåœºIsingæ¨¡å‹ (TFIM-2D)
- 1D Heisenbergæ¨¡å‹
- 1D XXZæ¨¡å‹  
- 1Dæ¨ªåœºIsingæ¨¡å‹ (TFIM-1D)
- å…¶ä»–é‡å­å¤šä½“æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate_quantum_dataset_quality.py <csv_file>

ç¤ºä¾‹ï¼š
    python evaluate_quantum_dataset_quality.py dataset_results/tfim_2d/n100|X(J,h,meas1000)_y(energy,entropy,corrs)_Nx3Ny3.csv
    python evaluate_quantum_dataset_quality.py dataset_results/heisenberg_1d/n100|X(coupling,meas128)_y(energy,entropy,corrs)_q127.csv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import ast
import sys
import re
from pathlib import Path

def parse_array_column(df, column_name):
    """è§£æå­˜å‚¨ä¸ºå­—ç¬¦ä¸²çš„æ•°ç»„åˆ—"""
    def parse_value(val):
        if not isinstance(val, str):
            return val
        if val.startswith('Any[') and val.endswith(']'):
            val = val[3:]
        try:
            return ast.literal_eval(val)
        except (ValueError, SyntaxError):
            return []
    
    return [parse_value(val) for val in df[column_name]]

def detect_model_type(csv_file):
    """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹"""
    file_path = Path(csv_file)
    
    if 'tfim_2d' in str(file_path):
        return 'tfim_2d'
    elif 'heisenberg_1d' in str(file_path):
        return 'heisenberg_1d'
    elif 'xxz_1d' in str(file_path):
        return 'xxz_1d'
    elif 'ising_1d' in str(file_path):
        return 'ising_1d'
    elif 'cluster_ising' in str(file_path):
        return 'cluster_ising'
    else:
        df = pd.read_csv(csv_file, nrows=1)
        columns = df.columns.tolist()
        
        if 'Nx' in columns and 'Ny' in columns:
            return 'tfim_2d'
        elif 'coupling_matrix' in columns:
            return 'heisenberg_1d'
        elif 'delta' in columns:
            return 'xxz_1d'
        elif 'J' in columns and 'qubits_num' in columns:
            return 'ising_1d'
        else:
            return 'unknown'

def get_model_quality_criteria(model_type, df):
    """è·å–æ¨¡å‹ç‰¹å®šçš„è´¨é‡è¯„ä¼°æ ‡å‡†"""
    criteria = {
        'energy_column': 'ground_state_energy',
        'entropy_columns': ['exact_entropy'],
        'correlation_columns': ['exact_correlation'],
        'parameter_columns': [],
        'system_size': 0,
        'energy_range': (-np.inf, 0),
        'entropy_range': (0, np.inf),
        'correlation_range': (-1, 1)
    }
    
    if model_type == 'tfim_2d':
        criteria.update({
            'energy_column': 'ground_state_energy',
            'entropy_columns': ['exact_entropy'],
            'correlation_columns': ['exact_correlation'],
            'parameter_columns': ['Nx', 'Ny', 'transverse_field', 'coupling_strength'],
            'system_size': df['Nx'].iloc[0] * df['Ny'].iloc[0] if 'Nx' in df.columns else 0,
            'energy_range': (-4 * criteria['system_size'], 0),
            'entropy_range': (0, np.log2(min(2**df['Nx'].iloc[0], 2**df['Ny'].iloc[0])) if 'Nx' in df.columns else (0, 10))
        })
        
    elif model_type == 'heisenberg_1d':
        criteria.update({
            'energy_column': 'ground_state_energy',
            'entropy_columns': ['exact_entropy'],
            'correlation_columns': ['exact_correlation'],
            'parameter_columns': ['coupling_matrix'],
            'system_size': len(parse_array_column(df, 'coupling_matrix')[0]) if 'coupling_matrix' in df.columns else 0,
            'energy_range': (-2 * criteria['system_size'], 0),
            'entropy_range': (0, np.log2(criteria['system_size']))
        })
        
    elif model_type == 'xxz_1d':
        criteria.update({
            'energy_column': 'energy',
            'entropy_columns': ['entropy'],
            'correlation_columns': ['corrzz'],
            'parameter_columns': ['nsites', 'delta'],
            'system_size': df['nsites'].iloc[0] if 'nsites' in df.columns else 0,
            'energy_range': (-2 * criteria['system_size'], 0),
            'entropy_range': (0, np.log2(criteria['system_size']))
        })
        
    elif model_type == 'ising_1d':
        criteria.update({
            'energy_column': 'energy',
            'entropy_columns': ['entropy'],
            'correlation_columns': ['corrzz'],
            'parameter_columns': ['qubits_num', 'J'],
            'system_size': df['qubits_num'].iloc[0] if 'qubits_num' in df.columns else 0,
            'energy_range': (-2 * criteria['system_size'], 0),
            'entropy_range': (0, np.log2(criteria['system_size']))
        })
    
    return criteria

def evaluate_physical_reasonableness(df, criteria):
    """è¯„ä¼°ç‰©ç†åˆç†æ€§"""
    print("ğŸ”¬ ç‰©ç†åˆç†æ€§æ£€æŸ¥")
    print("-" * 50)
    
    checks = []
    
    # 1. èƒ½é‡æ£€æŸ¥
    energy_col = criteria['energy_column']
    if energy_col in df.columns:
        energies = df[energy_col].values
        E_min, E_max = criteria['energy_range']
        
        print(f"ğŸ“Š èƒ½é‡ç»Ÿè®¡:")
        print(f"  èƒ½é‡èŒƒå›´: [{energies.min():.3f}, {energies.max():.3f}]")
        print(f"  ç†è®ºèŒƒå›´: [{E_min:.3f}, {E_max:.3f}]")
        
        if all(energies < 0):
            print("  âœ… æ‰€æœ‰èƒ½é‡ä¸ºè´Ÿæ•°ï¼ˆåŸºæ€ï¼‰")
            checks.append(True)
        else:
            print("  âŒ å­˜åœ¨æ­£èƒ½é‡")
            checks.append(False)
        
        if E_min != -np.inf and E_max != 0:
            if all(E_min <= energies) and all(energies <= E_max):
                print("  âœ… èƒ½é‡åœ¨åˆç†èŒƒå›´å†…")
                checks.append(True)
            else:
                print("  âš ï¸ èƒ½é‡è¶…å‡ºç†è®ºèŒƒå›´")
                checks.append(False)
        else:
            print("  âœ… èƒ½é‡èŒƒå›´æ£€æŸ¥è·³è¿‡ï¼ˆç†è®ºèŒƒå›´æœªå®šä¹‰ï¼‰")
            checks.append(True)
    
    # 2. å‚æ•°æ£€æŸ¥
    print(f"\nğŸ”§ å‚æ•°æ£€æŸ¥:")
    for param in criteria['parameter_columns']:
        if param in df.columns:
            if param in ['coupling_matrix', 'coupling_strength']:
                try:
                    param_data = parse_array_column(df, param)
                    if param_data:
                        flat_data = np.concatenate([np.array(p).flatten() for p in param_data if len(p) > 0])
                        if len(flat_data) > 0:
                            print(f"  {param}: [{flat_data.min():.3f}, {flat_data.max():.3f}]")
                            if all(flat_data >= 0):
                                print(f"    âœ… æ‰€æœ‰å€¼ä¸ºéè´Ÿæ•°")
                                checks.append(True)
                            else:
                                print(f"    âš ï¸ å­˜åœ¨è´Ÿå€¼")
                                checks.append(False)
                except:
                    pass
            else:
                values = df[param].values
                if values.dtype in ['object', 'string']:
                    print(f"  {param}: {values.tolist()}")
                    checks.append(True)  # å­—ç¬¦ä¸²ç±»å‹è·³è¿‡æ£€æŸ¥
                else:
                    print(f"  {param}: [{values.min():.3f}, {values.max():.3f}]")
                    if all(values > 0):
                        print(f"    âœ… æ‰€æœ‰å€¼ä¸ºæ­£æ•°")
                        checks.append(True)
                    else:
                        print(f"    âš ï¸ å­˜åœ¨éæ­£å€¼")
                        checks.append(False)
    
    return {'physical_checks': checks}

def evaluate_data_completeness(df):
    """è¯„ä¼°æ•°æ®å®Œæ•´æ€§"""
    print("\nğŸ“ˆ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
    print("-" * 50)
    
    n_samples = len(df)
    print(f"æ ·æœ¬æ•°é‡: {n_samples}")
    
    # æ ·æœ¬æ•°é‡è¯„ä¼°
    if n_samples < 10:
        print("âš ï¸ æ ·æœ¬å¤ªå°‘ï¼Œéš¾ä»¥è¿›è¡Œç»Ÿè®¡åˆ†æ")
        sample_quality = "poor"
    elif n_samples < 50:
        print("âœ… æ ·æœ¬æ•°é‡é€‚ä¸­ï¼Œé€‚åˆåˆæ­¥åˆ†æ")
        sample_quality = "moderate"
    elif n_samples < 200:
        print("âœ… æ ·æœ¬æ•°é‡å……è¶³ï¼Œé€‚åˆæœºå™¨å­¦ä¹ ")
        sample_quality = "good"
    else:
        print("âœ… æ ·æœ¬æ•°é‡ä¸°å¯Œï¼Œé€‚åˆæ·±åº¦ç ”ç©¶")
        sample_quality = "excellent"
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_data = df.isnull().sum()
    if missing_data.sum() == 0:
        print("âœ… æ— ç¼ºå¤±æ•°æ®")
        completeness = True
    else:
        print(f"âŒ å­˜åœ¨ç¼ºå¤±æ•°æ®: {missing_data.sum()} ä¸ª")
        completeness = False
    
    return {
        'n_samples': n_samples,
        'sample_quality': sample_quality,
        'completeness': completeness
    }

def evaluate_entanglement_quality(df, criteria):
    """è¯„ä¼°çº ç¼ ç†µè´¨é‡"""
    print("\nğŸ”— çº ç¼ ç†µè´¨é‡æ£€æŸ¥")
    print("-" * 50)
    
    entropy_cols = criteria['entropy_columns']
    valid_entropy_cols = [col for col in entropy_cols if col in df.columns]
    
    if not valid_entropy_cols:
        print("âš ï¸ æœªæ‰¾åˆ°çº ç¼ ç†µåˆ—")
        return {'entropy_checks': [False], 'error': 'No entropy columns found'}
    
    checks = []
    
    for col in valid_entropy_cols:
        try:
            entropy_data = parse_array_column(df, col)
            if entropy_data:
                avg_entropies = [np.mean(e) for e in entropy_data if len(e) > 0]
                if avg_entropies:
                    print(f"{col} ç»Ÿè®¡:")
                    print(f"  å¹³å‡ç†µèŒƒå›´: [{np.min(avg_entropies):.3f}, {np.max(avg_entropies):.3f}]")
                    print(f"  å¹³å‡ç†µå‡å€¼: {np.mean(avg_entropies):.3f}")
                    
                    # ç†µåˆç†æ€§æ£€æŸ¥
                    if all(np.array(avg_entropies) >= 0):
                        print("  âœ… æ‰€æœ‰ç†µå€¼éè´Ÿ")
                        checks.append(True)
                    else:
                        print("  âŒ å­˜åœ¨è´Ÿç†µå€¼")
                        checks.append(False)
                    
                    # ç†è®ºèŒƒå›´æ£€æŸ¥
                    S_min, S_max = criteria['entropy_range']
                    if S_max > 0:  # åªæœ‰å½“ç†è®ºèŒƒå›´æœ‰æ•ˆæ—¶æ‰æ£€æŸ¥
                        if all(S_min <= avg_entropies) and all(avg_entropies <= S_max):
                            print(f"  âœ… ç†µå€¼åœ¨ç†è®ºèŒƒå›´å†… [{S_min:.2f}, {S_max:.2f}]")
                            checks.append(True)
                        else:
                            print(f"  âš ï¸ ç†µå€¼è¶…å‡ºç†è®ºèŒƒå›´")
                            checks.append(False)
                    else:
                        print(f"  âœ… ç†µå€¼èŒƒå›´æ£€æŸ¥è·³è¿‡ï¼ˆç†è®ºèŒƒå›´æœªå®šä¹‰ï¼‰")
                        checks.append(True)
        except Exception as e:
            print(f"âš ï¸ çº ç¼ ç†µæ£€æŸ¥å¤±è´¥: {e}")
            checks.append(False)
    
    return {'entropy_checks': checks}

def evaluate_correlation_quality(df, criteria):
    """è¯„ä¼°å…³è”å‡½æ•°è´¨é‡"""
    print("\nğŸ”— å…³è”å‡½æ•°è´¨é‡æ£€æŸ¥")
    print("-" * 50)
    
    corr_cols = criteria['correlation_columns']
    valid_corr_cols = [col for col in corr_cols if col in df.columns]
    
    if not valid_corr_cols:
        print("âš ï¸ æœªæ‰¾åˆ°å…³è”å‡½æ•°åˆ—")
        return {'correlation_checks': [False], 'error': 'No correlation columns found'}
    
    checks = []
    
    for col in valid_corr_cols:
        try:
            corr_data = parse_array_column(df, col)
            if corr_data:
                flat_corr = np.concatenate([np.array(c).flatten() for c in corr_data if len(c) > 0])
                if len(flat_corr) > 0:
                    print(f"{col} ç»Ÿè®¡:")
                    print(f"  å…³è”å€¼èŒƒå›´: [{flat_corr.min():.3f}, {flat_corr.max():.3f}]")
                    print(f"  å…³è”å€¼å‡å€¼: {flat_corr.mean():.3f}")
                    
                    # å…³è”å‡½æ•°åˆç†æ€§æ£€æŸ¥
                    C_min, C_max = criteria['correlation_range']
                    if C_min != -1 or C_max != 1:  # åªæœ‰å½“èŒƒå›´ä¸æ˜¯é»˜è®¤çš„[-1,1]æ—¶æ‰æ£€æŸ¥
                        if all(C_min <= flat_corr) and all(flat_corr <= C_max):
                            print(f"  âœ… å…³è”å€¼åœ¨åˆç†èŒƒå›´å†… [{C_min}, {C_max}]")
                            checks.append(True)
                        else:
                            print(f"  âš ï¸ å…³è”å€¼è¶…å‡ºåˆç†èŒƒå›´")
                            checks.append(False)
                    else:
                        # å¯¹äº[-1,1]èŒƒå›´ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                        if all(-1.1 <= flat_corr) and all(flat_corr <= 1.1):  # å…è®¸å°çš„æ•°å€¼è¯¯å·®
                            print(f"  âœ… å…³è”å€¼åœ¨åˆç†èŒƒå›´å†…")
                            checks.append(True)
                        else:
                            print(f"  âš ï¸ å…³è”å€¼è¶…å‡ºåˆç†èŒƒå›´")
                            checks.append(False)
                    
                    # å¯¹ç§°æ€§æ£€æŸ¥ï¼ˆå¦‚æœæ˜¯çŸ©é˜µï¼‰
                    if len(corr_data[0]) > 1:
                        try:
                            # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å¯¹ç§°æ€§
                            first_corr = np.array(corr_data[0])
                            if first_corr.ndim == 2:
                                is_symmetric = np.allclose(first_corr, first_corr.T)
                                if is_symmetric:
                                    print("  âœ… å…³è”çŸ©é˜µå¯¹ç§°")
                                    checks.append(True)
                                else:
                                    print("  âš ï¸ å…³è”çŸ©é˜µä¸å¯¹ç§°")
                                    checks.append(False)
                        except:
                            pass
        except Exception as e:
            print(f"âš ï¸ å…³è”å‡½æ•°æ£€æŸ¥å¤±è´¥: {e}")
            checks.append(False)
    
    return {'correlation_checks': checks}

def evaluate_shadow_accuracy(df):
    """è¯„ä¼°Classical Shadowç²¾åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    print("\nğŸ¯ Classical Shadowç²¾åº¦è¯„ä¼°")
    print("-" * 50)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨approxåˆ—
    approx_cols = [col for col in df.columns if 'approx' in col.lower()]
    
    if not approx_cols:
        print("âš ï¸ æœªæ‰¾åˆ°Classical Shadowæ•°æ®")
        return {'accuracy_checks': [False], 'error': 'No shadow data found'}
    
    checks = []
    
    # å°è¯•è®¡ç®—ç²¾åº¦
    try:
        if 'approx_correlation' in df.columns and 'exact_correlation' in df.columns:
            exact_corrs = parse_array_column(df, 'exact_correlation')
            approx_corrs = parse_array_column(df, 'approx_correlation')
            
            exact_corr_flat = np.concatenate([np.array(c).flatten() for c in exact_corrs])
            approx_corr_flat = np.concatenate([np.array(c).flatten() for c in approx_corrs])
            
            corr_r = np.corrcoef(exact_corr_flat, approx_corr_flat)[0, 1]
            print(f"å…³è”å‡½æ•°ç²¾åº¦: R = {corr_r:.4f}")
            
            if corr_r > 0.8:
                print("  âœ… å…³è”å‡½æ•°ç²¾åº¦ä¼˜ç§€")
                checks.append(True)
            elif corr_r > 0.6:
                print("  âœ… å…³è”å‡½æ•°ç²¾åº¦è‰¯å¥½")
                checks.append(True)
            else:
                print("  âš ï¸ å…³è”å‡½æ•°ç²¾åº¦éœ€è¦æ”¹è¿›")
                checks.append(False)
        
        if 'approx_entropy' in df.columns and 'exact_entropy' in df.columns:
            exact_entropies = parse_array_column(df, 'exact_entropy')
            approx_entropies = parse_array_column(df, 'approx_entropy')
            
            exact_entropy_flat = np.concatenate([np.array(e).flatten() for e in exact_entropies])
            approx_entropy_flat = np.concatenate([np.array(e).flatten() for e in approx_entropies])
            
            entropy_r = np.corrcoef(exact_entropy_flat, approx_entropy_flat)[0, 1]
            print(f"ç†µç²¾åº¦: R = {entropy_r:.4f}")
            
            if entropy_r > 0.7:
                print("  âœ… ç†µç²¾åº¦ä¼˜ç§€")
                checks.append(True)
            elif entropy_r > 0.5:
                print("  âœ… ç†µç²¾åº¦è‰¯å¥½")
                checks.append(True)
            else:
                print("  âš ï¸ ç†µç²¾åº¦éœ€è¦æ”¹è¿›")
                checks.append(False)
    
    except Exception as e:
        print(f"âš ï¸ ç²¾åº¦è¯„ä¼°å¤±è´¥: {e}")
        checks.append(False)
    
    return {'accuracy_checks': checks}

def generate_quality_report(results, model_type):
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ æ•°æ®é›†è´¨é‡æŠ¥å‘Š")
    print("="*60)
    
    # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
    total_checks = 0
    passed_checks = 0
    
    # ç‰©ç†åˆç†æ€§
    if 'physical' in results and 'physical_checks' in results['physical']:
        total_checks += len(results['physical']['physical_checks'])
        passed_checks += sum(results['physical']['physical_checks'])
    
    # æ•°æ®å®Œæ•´æ€§
    if results['completeness']['completeness']:
        passed_checks += 1
    total_checks += 1
    
    # çº ç¼ ç†µ
    if 'entanglement' in results and 'entropy_checks' in results['entanglement']:
        total_checks += len(results['entanglement']['entropy_checks'])
        passed_checks += sum(results['entanglement']['entropy_checks'])
    
    # å…³è”å‡½æ•°
    if 'correlation' in results and 'correlation_checks' in results['correlation']:
        total_checks += len(results['correlation']['correlation_checks'])
        passed_checks += sum(results['correlation']['correlation_checks'])
    
    # ç²¾åº¦
    if 'shadow' in results and 'accuracy_checks' in results['shadow']:
        total_checks += len(results['shadow']['accuracy_checks'])
        passed_checks += sum(results['shadow']['accuracy_checks'])
    
    quality_score = passed_checks / total_checks if total_checks > 0 else 0
    
    print(f"\næ€»ä½“è´¨é‡åˆ†æ•°: {quality_score:.1%} ({passed_checks}/{total_checks})")
    
    if quality_score >= 0.9:
        print("ğŸ† æ•°æ®é›†è´¨é‡ä¼˜ç§€ï¼")
    elif quality_score >= 0.7:
        print("âœ… æ•°æ®é›†è´¨é‡è‰¯å¥½")
    elif quality_score >= 0.5:
        print("âš ï¸ æ•°æ®é›†è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ”¹è¿›")
    else:
        print("âŒ æ•°æ®é›†è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
    
    # æ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    if results['completeness']['n_samples'] < 50:
        print("  - å¢åŠ æ ·æœ¬æ•°é‡åˆ°50+")
    
    if 'shadow' in results and results['shadow'].get('accuracy_checks', [False]) and not all(results['shadow']['accuracy_checks']):
        print("  - å¢åŠ Classical Shadowæµ‹é‡æ¬¡æ•°")
    
    if 'physical' in results and not all(results['physical'].get('physical_checks', [True])):
        print("  - æ£€æŸ¥ç‰©ç†å‚æ•°è®¾ç½®")
    
    print(f"\næ¨¡å‹ç±»å‹: {model_type}")
    print("\n" + "="*60)

def main():
    if len(sys.argv) < 2:
        print("Usage: python evaluate_quantum_dataset_quality.py <csv_file>")
        print("\nSupported models:")
        print("  - 2Dæ¨ªåœºIsingæ¨¡å‹ (TFIM-2D)")
        print("  - 1D Heisenbergæ¨¡å‹")
        print("  - 1D XXZæ¨¡å‹")
        print("  - 1Dæ¨ªåœºIsingæ¨¡å‹ (TFIM-1D)")
        print("  - å…¶ä»–é‡å­å¤šä½“æ¨¡å‹")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    
    if not Path(csv_file).exists():
        print(f"Error: File not found: {csv_file}")
        sys.exit(1)
    
    print("="*60)
    print("é€šç”¨é‡å­å¤šä½“æ•°æ®é›†è´¨é‡è¯„ä¼°")
    print("="*60)
    print(f"\nåˆ†ææ–‡ä»¶: {csv_file}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(csv_file)
    print(f"âœ“ åŠ è½½ {len(df)} ä¸ªæ ·æœ¬")
    
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    model_type = detect_model_type(csv_file)
    print(f"âœ“ æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
    
    # è·å–è´¨é‡è¯„ä¼°æ ‡å‡†
    criteria = get_model_quality_criteria(model_type, df)
    print(f"âœ“ ç³»ç»Ÿå¤§å°: {criteria['system_size']}")
    
    # æ‰§è¡Œå„é¡¹è¯„ä¼°
    results = {}
    
    results['physical'] = evaluate_physical_reasonableness(df, criteria)
    results['completeness'] = evaluate_data_completeness(df)
    results['entanglement'] = evaluate_entanglement_quality(df, criteria)
    results['correlation'] = evaluate_correlation_quality(df, criteria)
    results['shadow'] = evaluate_shadow_accuracy(df)
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    generate_quality_report(results, model_type)

if __name__ == "__main__":
    main()
